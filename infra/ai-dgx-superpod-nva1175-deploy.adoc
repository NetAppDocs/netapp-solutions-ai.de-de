---
sidebar: sidebar 
permalink: infra/ai-dgx-superpod-nva1175-deploy.html 
keywords: NetApp AI, AI, Artificial Intelligence, ML, Machine Learning, NVIDIA, NVIDIA AI Enterprise, NVIDIA SuperPOD, NVIDIA DGX 
summary: NVIDIA DGX SuperPOD mit NetApp AFF A90 
---
= NetApp AFF A90 -Speichersysteme mit NVIDIA DGX SuperPOD
:allow-uri-read: 




== NVA-Bereitstellung

[role="lead"]
Der NVIDIA DGX SuperPOD mit NetApp AFF A90 Speichersystemen kombiniert die erstklassige Rechenleistung von NVIDIA DGX-Systemen mit Cloud-verbundenen NetApp -Speichersystemen, um datengesteuerte Workflows für maschinelles Lernen (ML), künstliche Intelligenz (KI) und High-Performance Technical Computing (HPC) zu ermöglichen.  Dieses Dokument beschreibt die Konfigurations- und Bereitstellungsdetails für die Integration von AFF A90 Speichersystemen in die DGX SuperPOD-Architektur.

image:nvidialogo.png["nvidia Logo"]

David Arnette, NetApp



== Programmübersicht

NVIDIA DGX SuperPOD™ bietet eine schlüsselfertige KI-Rechenzentrumslösung für Unternehmen und liefert nahtlos erstklassiges Computing, Softwaretools, Fachwissen und kontinuierliche Innovation.  DGX SuperPOD bietet alles, was Kunden benötigen, um KI/ML- und HPC-Workloads mit minimaler Einrichtungszeit und maximaler Produktivität bereitzustellen.  Abbildung 1 zeigt die High-Level-Komponenten von DGX SuperPOD.

Abbildung 1) NVIDIA DGX SuperPOD mit NetApp AFF A90 Speichersystemen.

image:ai-superpod-a90-005.png["600.600"]

DGX SuperPOD bietet die folgenden Vorteile:

* Bewährte Leistung für KI/ML- und HPC-Workloads
* Integrierter Hardware- und Software-Stack von Infrastrukturmanagement und -überwachung bis hin zu vorgefertigten Deep-Learning-Modellen und -Tools.
* Spezielle Dienste von der Installation und Infrastrukturverwaltung bis hin zur Skalierung von Workloads und Optimierung der Produktions-KI.




== Lösungsübersicht

Da Unternehmen Initiativen für künstliche Intelligenz (KI) und maschinelles Lernen (ML) ergreifen, war die Nachfrage nach robusten, skalierbaren und effizienten Infrastrukturlösungen noch nie so groß.  Im Mittelpunkt dieser Initiativen steht die Herausforderung, immer komplexere KI-Modelle zu verwalten und zu trainieren und gleichzeitig Datensicherheit, Zugänglichkeit und Ressourcenoptimierung zu gewährleisten. 

Diese Lösung bietet die folgenden Hauptvorteile:

* *Skalierbarkeit*
* *Datenverwaltung und -zugriff*
* *Sicherheit*




=== Lösungstechnologie

NVIDIA DGX SuperPOD umfasst die Server, Netzwerke und Speicher, die für die Bereitstellung bewährter Leistung für anspruchsvolle KI-Workloads erforderlich sind.  NVIDIA DGX™ H200- und B200-Systeme bieten Rechenleistung der Weltklasse und NVIDIA Quantum InfiniBand- und Spectrum™-Ethernet-Netzwerk-Switches bieten extrem niedrige Latenz und branchenführende Netzwerkleistung.  Durch die Ergänzung der branchenführenden Datenverwaltungs- und Leistungsfunktionen des NetApp ONTAP Speichers können Kunden KI/ML-Initiativen schneller und mit geringerem Datenmigrations- und Verwaltungsaufwand umsetzen.  Weitere Informationen zu den spezifischen Komponenten dieser Lösung finden Sie imhttps://www.netapp.com/pdf.html?item=/media/125003-nva-1175-design-superpod-a90.pdf["NVA-1175-Designhandbuch"] Und https://docs.nvidia.com/dgx-superpod/reference-architecture-scalable-infrastructure-b200/latest/index.html["+++ NVIDIA DGX SuperPOD Referenzarchitektur+++"] Dokumentation.



=== Zusammenfassung des Anwendungsfalls

NVIDIA DGX SuperPOD wurde entwickelt, um die Leistungs- und Skalierungsanforderungen der anspruchsvollsten Workloads zu erfüllen.

Diese Lösung gilt für die folgenden Anwendungsfälle:

* Maschinelles Lernen im großen Maßstab mithilfe herkömmlicher Analysetools.
* Training künstlicher Intelligenzmodelle für große Sprachmodelle, Computervision/Bildklassifizierung, Betrugserkennung und unzählige andere Anwendungsfälle.
* Hochleistungsrechnen wie seismische Analyse, numerische Strömungsmechanik und großflächige Visualisierung.




== Technologieanforderungen

DGX SuperPOD basiert auf dem Konzept einer skalierbaren Einheit (SU), die alle erforderlichen Komponenten enthält, um die erforderliche Konnektivität und Leistung bereitzustellen und etwaige Engpässe in der Infrastruktur zu beseitigen.  Kunden können mit einer oder mehreren SUs beginnen und nach Bedarf weitere SUs hinzufügen, um ihre Anforderungen zu erfüllen.  Weitere Informationen finden Sie im https://docs.nvidia.com/dgx-superpod/reference-architecture-scalable-infrastructure-b200/latest/index.html["+++ NVIDIA DGX SuperPOD Referenzarchitektur+++"] .  Dieses Dokument beschreibt die Speicherkomponenten und die Konfiguration für eine einzelne SU.



=== Hardwareanforderungen

Tabelle 1 listet die Hardwarekomponenten auf, die zur Implementierung der Speicherkomponenten für 1SU erforderlich sind.  Spezifische Teile und Mengen für 1–4 skalierbare Einheiten finden Sie in Anhang A.

Tabelle 1) Hardwareanforderungen.

[cols="50%,50%"]
|===
| Hardware | Menge 


| NetApp AFF A90 Speichersystem | 4 


| NetApp Storage Cluster Interconnect-Switch | 2 


| NVIDIA 800 GB -> 4 x 200 GB Splitterkabel | 12 
|===


=== Softwareanforderungen

Tabelle 2 listet die Mindestsoftwarekomponenten und -versionen auf, die zur Integration des AFF A90 -Speichersystems mit DGX SuperPOD erforderlich sind.  DGX SuperPOD umfasst auch andere Softwarekomponenten, die hier nicht aufgeführt sind.  Bitte beachten Sie diehttps://docs.nvidia.com/dgx-superpod/release-notes/latest/10-24-11.html["+++DGX SuperPOD-Versionshinweise+++"] für vollständige Details.

Tabelle 2) Softwareanforderungen.

[cols="50%,50%"]
|===
| Software | Version 


| NetApp ONTAP | 9.16.1 oder höher 


| NVIDIA BaseCommand Manager | 10.24.11 oder höher 


| NVIDIA DGX-Betriebssystem | 6.3.1 oder höher 


| NVIDIA OFED-Treiber | MLNX_OFED_LINUX-23.10.3.2.0 LTS oder höher 


| NVIDIA Cumulus OS | 5.10 oder höher 
|===


== Bereitstellungsverfahren

Die Integration von NetApp ONTAP -Speicher mit DGX SuperPOD umfasst die folgenden Aufgaben:

* Netzwerkkonfiguration für NetApp AFF A90 Storage-Systeme mit RoCE
* Installation und Konfiguration des Speichersystems
* DGX-Clientkonfiguration mit NVIDIA Base Command™ Manager




=== Installation und Konfiguration des Speichersystems



==== Standortvorbereitung und Grundinstallation

Die Standortvorbereitung und die Basisinstallation des AFF A90 Speicherclusters werden von NetApp Professional Services für alle DGX SuperPOD-Bereitstellungen im Rahmen des Standardbereitstellungsdienstes durchgeführt.  NetApp PS bestätigt, dass die Standortbedingungen für die Installation geeignet sind, und installiert die Hardware in den dafür vorgesehenen Racks.  Sie stellen außerdem die OOB-Netzwerkverbindungen her und führen die grundlegende Clustereinrichtung mithilfe der vom Kunden bereitgestellten Netzwerkinformationen durch.  Anhang A – Stückliste und Rack-Erhöhungen enthält standardmäßige Rack-Erhöhungen als Referenz.  Weitere Informationen zur A90-Installation finden Sie im https://docs.netapp.com/us-en/ontap-systems/a70-90/install-overview.html["+++ AFF A90 Hardware-Installationsdokumentation+++"] .

Sobald die Standardbereitstellung abgeschlossen ist, führt NetApp PS die erweiterte Konfiguration der Speicherlösung mithilfe der folgenden Verfahren durch, einschließlich der Integration mit Base Command Manager für Clientkonnektivität und -optimierung.



==== Verkabeln des Speichersystems mit dem DGX SuperPOD-Speichergewebe

Das AFF A90 Speichersystem ist über vier 200-GB-Ethernet-Ports pro Controller mit den Storage Fabric Leaf-Switches verbunden, mit zwei Verbindungen zu jedem Switch.  Die 800-GB-Switch-Ports der NVIDIA Spectrum SN5600-Switches werden mithilfe der entsprechenden DAC- oder optischen Splitterkonfigurationen, die in Anhang A aufgeführt sind, in 4 x 200-GB-Ports aufgeteilt. Die einzelnen Ports jedes Switch-Ports werden über den Speichercontroller verteilt, um einzelne Fehlerquellen zu vermeiden.  Abbildung 2 unten zeigt die Verkabelung für die Storage Fabric-Verbindungen:

Abbildung 2) Speichernetzwerkverkabelung.

image:ai-superpod-a90-006.png["600.600"]



==== Verkabeln des Speichersystems mit dem DGX SuperPOD In-Band-Netzwerk

NetApp ONTAP umfasst branchenführende Multi-Tenancy-Funktionen, die es ermöglichen, sowohl als Hochleistungsspeichersystem in der DGX SuperPOD-Architektur zu fungieren als auch Home-Verzeichnisse, Gruppendateifreigaben und Base Command Manager-Cluster-Artefakte zu unterstützen.  Für die Verwendung im In-Band-Netzwerk ist jeder AFF A90 Controller mit einer 200-Gb-Ethernet-Verbindung pro Controller mit den In-Band-Netzwerk-Switches verbunden und die Ports sind in einer LACP-MLAG-Konfiguration konfiguriert.  Abbildung 3 unten zeigt die Verkabelung des Speichersystems mit den In-Band- und OOB-Netzwerken.

Abbildung 3) In-Band- und OOB-Netzwerkverkabelung.

image:ai-superpod-a90-007.png["600.600"]



==== Konfigurieren Sie ONTAP für DGX SuperPOD

Diese Lösung nutzt mehrere Storage Virtual Machines (SVM), um Volumes sowohl für den Hochleistungsspeicherzugriff als auch für Benutzer-Home-Verzeichnisse und andere Cluster-Artefakte auf einer Verwaltungs-SVM zu hosten.  Jede SVM ist mit Netzwerkschnittstellen im Speicher oder In-Band-Netzwerken und FlexGroup -Volumes zur Datenspeicherung konfiguriert.  Um die Leistung für die Daten-SVM sicherzustellen, wird eine Speicher-QoS-Richtlinie implementiert.  Weitere Informationen zu FlexGroups, Storage Virtual Machines und ONTAP QoS-Funktionen finden Sie im https://docs.netapp.com/us-en/ontap/index.html["+++ ONTAP Dokumentation+++"] .



===== Konfigurieren des Basisspeichers



====== Konfigurieren Sie auf jedem Controller ein einzelnes Aggregat

[source, cli]
----
aggr create -node <node> -aggregate <node>_data01 -diskcount <47> -maxraidsize 24
----
Wiederholen Sie die obigen Schritte für jeden Knoten im Cluster.



====== Konfigurieren Sie ifgrps auf jedem Controller für das In-Band-Netzwerk

[source, cli]
----
net port ifgrp create -node <node> -ifgrp a1a -mode multimode
-distr-function port

net port ifgrp add-port -node <node> -ifgrp a1a -ports
<node>:e2a,<node>:e2b
----
Wiederholen Sie die obigen Schritte für jeden Knoten im Cluster.



====== Konfigurieren physischer Ports für RoCE

Die Aktivierung von NFS über RDMA erfordert eine Konfiguration, um sicherzustellen, dass der Netzwerkverkehr sowohl auf dem Client als auch auf dem Server entsprechend gekennzeichnet und dann vom Netzwerk mithilfe von RDMA über Converged Ethernet (RoCE) entsprechend verarbeitet wird.  Dazu gehört die Konfiguration der Priority Flow Control (PFC) und die Konfiguration der zu verwendenden PFC-CoS-Warteschlange.  NetApp ONTAP konfiguriert den DSCP-Code 26 außerdem automatisch, um ihn an die QoS-Konfiguration des Netzwerks anzupassen, wenn die folgenden Befehle ausgeführt werden.

[source, cli]
----
network port modify -node * -port e6* -flowcontrol-admin pfc
-pfc-queues-admin 3

network port modify -node * -port e11* -flowcontrol-admin pfc
-pfc-queues-admin 3
----


====== Erstellen von Broadcastdomänen

[source, cli]
----
broadcast-domain create -broadcast-domain in-band -mtu 9000 -ports
ntapa90_spod-01:a1a,ntapa90_spod-02:a1a,ntapa90_spod-03:a1a,ntapa90_spod-04:a1a,ntapa90_spod-05:a1a,
ntapa90_spod-06:a1a,ntapa90_spod-07:a1a,ntapa90_spod-08:a1a

broadcast-domain create -broadcast-domain vlan401 -mtu 9000 -ports
ntapa90_spod-01:e6a,ntapa90_spod-01:e6b,ntapa90_spod-02:e6a,ntapa90_spod-02:e6b,ntapa90_spod-03:e6a,ntapa90_spod-03:e6b,ntapa90_spod-04:e6a,ntapa90_spod-04:e6b,ntapa90_spod-05:e6a,ntapa90_spod-05:e6b,ntapa90_spod-06:e6a,ntapa90_spod-06:e6b,ntapa90_spod-07:e6a,ntapa90_spod-07:e6b,ntapa90_spod-08:e6a,ntapa90_spod-08:e6b

broadcast-domain create -broadcast-domain vlan402 -mtu 9000 -ports
ntapa90_spod-01:e11a,ntapa90_spod-01:e11b,ntapa90_spod-02:e11a,ntapa90_spod-02:e11b,ntapa90_spod-03:e11a,ntapa90_spod-03:e11b,ntapa90_spod-04:e11a,ntapa90_spod-04:e11b,ntapa90_spod-05:e11a,ntapa90_spod-05:e11b,ntapa90_spod-06:e11a,ntapa90_spod-06:e11b,ntapa90_spod-07:e11a,ntapa90_spod-07:e11b,ntapa90_spod-08:e11a,ntapa90_spod-08:e11b

----


===== Management-SVM erstellen



====== Erstellen und Konfigurieren der Management-SVM

[source, cli]
----
vserver create -vserver spod_mgmt

vserver modify -vserver spod_mgmt -aggr-list
ntapa90_spod-01_data01,ntapa90_spod-02_data01,
ntapa90_spod-03_data01,ntapa90_spod-04_data01,
ntapa90_spod-05_data01,ntapa90_spod-06_data01,
ntapa90_spod-07_data01,ntapa90_spod-08_data01
----


====== Konfigurieren Sie den NFS-Dienst auf der Management-SVM

[source, cli]
----
nfs create -vserver spod_mgmt -v3 enabled -v4.1 enabled -v4.1-pnfs
enabled -tcp-max-xfer-size 262144 -v4.1-trunking enabled

set advanced

nfs modify -vserver spod_mgmt -v3-64bit-identifiers enabled
-v4.x-session-num-slots 1024
----


====== Erstellen Sie IP-Subnetze für In-Band-Netzwerkschnittstellen

[source, cli]
----
network subnet create -subnet-name inband -broadcast-domain in-band
-subnet xxx.xxx.xxx.0/24 -gateway xxx.xxx.xxx.x -ip-ranges
xxx.xxx.xxx.xx-xxx.xxx.xxx.xxx
----
*Hinweis:* IP-Subnetzinformationen müssen vom Kunden zum Zeitpunkt der Bereitstellung zur Integration in bestehende Kundennetzwerke bereitgestellt werden.



====== Erstellen Sie auf jedem Knoten Netzwerkschnittstellen für In-Band-SVM

[source, cli]
----
net int create -vserver spod_mgmt -lif inband_lif1 -home-node
ntapa90_spod-01 -home-port a1a -subnet_name inband
----
Wiederholen Sie die obigen Schritte für jeden Knoten im Cluster.



====== Erstellen Sie FlexGroup -Volumes für Management-SVM

[source, cli]
----
vol create -vserver spod_mgmt -volume home -size 10T -auto-provision-as
flexgroup -junction-path /home

vol create -vserver spod_mgmt -volume cm -size 10T -auto-provision-as
flexgroup -junction-path /cm

----


====== Exportrichtlinie für Management-SVM erstellen

[source, cli]
----
export-policy rule create -vserver spod_mgmt -policy default
-client-match XXX.XXX.XXX.XXX -rorule sys -rwrule sys -superuser sys
----
*Hinweis:* IP-Subnetzinformationen müssen vom Kunden zum Zeitpunkt der Bereitstellung zur Integration in bestehende Kundennetzwerke bereitgestellt werden.



===== Daten-SVM erstellen



====== Erstellen und Konfigurieren von Data SVM

[source, cli]
----
vserver create -vserver spod_data
vserver modify -vserver spod_data -aggr-list
ntapa90_spod-01_data01,ntapa90_spod-02_data01,
ntapa90_spod-03_data01,ntapa90_spod-04_data01,
ntapa90_spod-05_data01,ntapa90_spod-06_data01,
ntapa90_spod-07_data01,ntapa90_spod-08_data01
----


====== Konfigurieren Sie den NFS-Dienst auf Data SVM mit aktiviertem RDMA

[source, cli]
----
nfs create -vserver spod_data -v3 enabled -v4.1 enabled -v4.1-pnfs
enabled -tcp-max-xfer-size 262144 -v4.1-trunking enabled -rdma enabled

set advanced

nfs modify -vserver spod_data -v3-64bit-identifiers enabled
-v4.x-session-num-slots 1024
----


====== Erstellen Sie IP-Subnetze für Data SVM-Netzwerkschnittstellen

[source, cli]
----
network subnet create -subnet-name vlan401 -broadcast-domain vlan401
-subnet 100.127.124.0/24 -ip-ranges 100.127.124.4-100.127.124.254

network subnet create -subnet-name vlan402 -broadcast-domain vlan402
-subnet 100.127.252.0/24 -ip-ranges 100.127.252.4-100.127.252.254
----


====== Erstellen Sie auf jedem Knoten Netzwerkschnittstellen für Data SVM

[source, cli]
----
net int create -vserver spod_data -lif data_lif1 -home-node
ntapa90_spod-01 -home-port e6a -subnet_name vlan401 -failover-policy
sfo-partner-only

net int create -vserver spod_data -lif data_lif2 -home-node
ntapa90_spod-01 -home-port e6b -subnet_name vlan401

net int create -vserver spod_data -lif data_lif3 -home-node
ntapa90_spod-01 -home-port e11a -subnet_name vlan402

net int create -vserver spod_data -lif data_lif4 -home-node
ntapa90_spod-01 -home-port e11b -subnet_name vlan402

----
Wiederholen Sie die obigen Schritte für jeden Knoten im Cluster.



====== Konfigurieren von Data SVM-Netzwerkschnittstellen für RDMA

[source, cli]
----
net int modify -vserver spod_data -lif * -rdma-protocols roce
----


====== Erstellen einer Exportrichtlinie für Daten-SVM

[source, cli]
----
export-policy rule create -vserver spod_data -policy default
-client-match 100.127.0.0/16 -rorule sys -rwrule sys -superuser sys
----


====== Erstellen Sie statische Routen auf Daten-SVM

[source, cli]
----
route add -vserver spod_data -destination 100.127.0.0/17 -gateway
100.127.124.1 -metric 20

route add -vserver spod_data -destination 100.127.0.0/17 -gateway
100.127.252.1 -metric 30

route add -vserver spod_data -destination 100.127.128.0/17 -gateway
100.127.252.1 -metric 20

route add -vserver spod_data -destination 100.127.128.0/17 -gateway
100.127.124.1 -metric 30
----


====== Erstellen Sie ein FlexGroup -Volume mit GDD für Data SVM

Granular Data Distribution (GDD) ermöglicht die Verteilung großer Datendateien auf mehrere FlexGroup -Volumes und -Controller, um maximale Leistung für Einzeldatei-Workloads zu erzielen.  NetApp empfiehlt, GDD auf Datenvolumes für alle DGX SuperPOD-Bereitstellungen zu aktivieren.

[source, cli]
----
set adv

vol create -vserver spod-data -volume spod_data -size 100T -aggr-list
ntapa90_spod-01_data01,ntapa90_spod-02_data01,
ntapa90_spod-03_data01,ntapa90_spod-04_data01,
ntapa90_spod-05_data01,ntapa90_spod-06_data01,
ntapa90_spod-07_data01,ntapa90_spod-08_data01 -aggr-multiplier 16
-granular-data advanced -junction-path /spod_data  
----


====== Deaktivieren der Speichereffizienz für das primäre Datenvolumen

Volume-Effizienz aus -vserver spod_data -volume spod_data



====== Erstellen Sie eine QoS-Mindestrichtlinie für Daten-SVM

[source, cli]
----
qos policy-group create -policy-group spod_qos -vserver spod_data
-min-throughput 62GB/s -is-shared true
----


====== Wenden Sie die QoS-Richtlinie für Daten-SVM an

[source, cli]
----
Volume modify -vserver spod_data -volume spod_data -qos-policy-group
spod_qos
----


=== DGX-Serverkonfiguration mit NVIDIA Base Command Manager

Führen Sie die folgenden Aufgaben aus, um die DGX-Clients für die Verwendung des AFF A90 -Speichersystems vorzubereiten.  Dieser Prozess setzt voraus, dass Netzwerkschnittstellen und statische Routen für das Speicher-Fabric bereits auf den DGX-Systemknoten konfiguriert wurden.  Die folgenden Aufgaben werden von NetApp Professional Services im Rahmen des erweiterten Konfigurationsprozesses ausgeführt.



==== Konfigurieren Sie das DGX-Server-Image mit den erforderlichen Kernelparametern und anderen Einstellungen

NetApp ONTAP verwendet branchenübliche NFS-Protokolle und erfordert keine Installation zusätzlicher Software auf den DGX-Systemen.  Um eine optimale Leistung der Clientsysteme zu erzielen, sind mehrere Änderungen am DGX-Systemabbild erforderlich.  Die beiden folgenden Schritte werden ausgeführt, nachdem Sie mit dem folgenden Befehl in den Chroot-Modus des BCM-Images gewechselt sind:

[source, cli]
----
cm-chroot-sw-img /cm/images/<image>
----


===== Konfigurieren Sie die Einstellungen für den virtuellen Systemspeicher in /etc/sysctl.conf

Die Standardkonfiguration des Linux-Systems bietet virtuelle Speichereinstellungen, die möglicherweise nicht unbedingt eine optimale Leistung liefern.  Bei DGX B200-Systemen mit 2 TB RAM sind in den Standardeinstellungen 40 GB Pufferspeicher zulässig, was zu inkonsistenten E/A-Mustern führt und es dem Client ermöglicht, das Speichersystem beim Leeren des Puffers zu überlasten.  Die folgenden Einstellungen begrenzen den Client-Pufferspeicher auf 5 GB und erzwingen häufigeres Leeren, um einen konsistenten E/A-Stream zu erstellen, der das Speichersystem nicht überlastet.

Nachdem Sie in den Chroot-Modus des Images gewechselt sind, bearbeiten Sie die Datei /etc/sysctl.s/90-cm-sysctl.conf und fügen Sie die folgenden Zeilen hinzu:

[source, cli]
----
vm.dirty_ratio=0 #controls max host RAM used for buffering as a
percentage of total RAM, when this limit is reached all applications
must flush buffers to continue

vm.dirty_background_ratio=0 #controls low-watermark threshold to start
background flushing as a percentage of total RAM

vm.dirty_bytes=5368709120 #controls max host RAM used for buffering as
an absolute value (note _ratio above only accepts integers and the value
we need is <1% of total RAM (2TB))

vm.dirty_background_bytes=2147483648 #controls low-watermark threshold
to start background flushing as an absolute value

vm.dirty_expire_centisecs = 300 #controls how long data remains in
buffer pages before being marked dirty

vm.dirty_writeback_centisecs = 100 #controls how frequently the flushing
process wakes up to flush dirty buffers
----
Speichern und schließen Sie die Datei /etc/sysctl.conf.



===== Konfigurieren Sie andere Systemeinstellungen mit einem Skript, das nach dem Neustart ausgeführt wird

Für die Ausführung einiger Einstellungen ist es erforderlich, dass das Betriebssystem vollständig online ist. Diese Einstellungen bleiben nach einem Neustart nicht bestehen.  Um diese Einstellungen in einer Base Command Manager-Umgebung vorzunehmen, erstellen Sie eine Datei /root/ntap_dgx_config.sh und geben Sie die folgenden Zeilen ein:

[source, cli]
----
#!/bin/bash

##The commands below are platform-specific based.

##For H100/H200 systems use the following variables

## NIC1_ethname= enp170s0f0np0

## NIC1_pciname=aa:00.0

## NCI1_mlxname=mlx5_7

## NIC1_ethname= enp41s0f0np0

## NIC1_pciname=29:00.0

## NCI1_mlxname=mlx5_1

##For B200 systems use the following variables

NIC1_ethname=enp170s0f0np0

NIC1_pciname=aa:00.0

NCI1_mlxname=mlx5_11

NIC2_ethname=enp41s0f0np0

NIC2_pciname=29:00.0

NCI2_mlxname=mlx5_5

mstconfig -y -d $\{NIC1_pciname} set ADVANCED_PCI_SETTINGS=1
NUM_OF_VFS=0

mstconfig -y -d $\{NIC2_pciname} set ADVANCED_PCI_SETTINGS=1
NUM_OF_VFS=0

setpci -s $\{NIC1_pciname} 68.W=5957

setpci -s $\{NIC2_pciname} 68.W=5957

ethtool -G $\{NIC1_ethname} rx 8192 tx 8192

ethtool -G $\{NIC2_ethname} rx 8192 tx 8192

mlnx_qos -i $\{NIC1_ethname} --pfc 0,0,0,1,0,0,0,0 --trust=dscp

mlnx_qos -i $\{NIC2_ethname} --pfc 0,0,0,1,0,0,0,0 --trust=dscp

echo 106 > /sys/class/infiniband/$\{NIC1_mlxname}/tc/1/traffic_class

echo 106 > /sys/class/infiniband/$\{NIC2_mlxname}/tc/1/traffic_class
----
*Speichern und schließen Sie die Datei.  Ändern Sie die Berechtigungen für die Datei, sodass sie ausführbar ist:*

[source, cli]
----
chmod 755 /root/ntap_dgx_config.sh
----
Erstellen Sie einen Cron-Job, der beim Booten von Root ausgeführt wird, indem Sie die folgende Zeile bearbeiten:

[source, cli]
----
@reboot /root/ntap_dgx_config.sh
----
Siehe die Beispiel-Crontab-Datei unten:

[source, cli]
----
# Edit this file to introduce tasks to be run by cron.

#

# Each task to run has to be defined through a single line

# indicating with different fields when the task will be run

# and what command to run for the task

#

# To define the time you can provide concrete values for

# minute (m), hour (h), day of month (dom), month (mon),

# and day of week (dow) or use '*' in these fields (for 'any').

#

# Notice that tasks will be started based on the cron's system

# daemon's notion of time and timezones.

#

# Output of the crontab jobs (including errors) is sent through

# email to the user the crontab file belongs to (unless redirected).

#

# For example, you can run a backup of all your user accounts

# at 5 a.m every week with:

# 0 5 * * 1 tar -zcf /var/backups/home.tgz /home/

#

# For more information see the manual pages of crontab(5) and cron(8)

#

# m h dom mon dow command

@reboot /home/ntap_dgx_config.sh
----
Verlassen Sie den Chroot-Modus des BCM-Images, indem Sie „exit“ oder „Strg+D“ eingeben.



==== Konfigurieren Sie die DGX-Kategorie von BaseCommand Manager für Client-Mount-Punkte

Um die DGX-Clients für die Einbindung des AFF A90 Speichersystems zu konfigurieren, muss die von den DGX-Systemen verwendete BCM-Clientkategorie geändert werden, um die relevanten Informationen und Optionen einzuschließen.  Die folgenden Schritte beschreiben, wie der NFS-Mountpunkt konfiguriert wird.

[source, cli]
----
cmsh

category ; use category <category>; fsmounts

add superpod

set device 100.127.124.4:/superpod

set mountpoint /mnt/superpod

set filesystem nfs

set mountoptions
vers=4.1,proto=rdma,max_connect=16,write=eager,rsize=262144,wsize=262144

commit
----


== Abschluss

Der NVIDIA DGX SuperPOD mit NetApp * AFF A90 -Speichersystemen* stellt einen bedeutenden Fortschritt bei KI-Infrastrukturlösungen dar.  Durch die Bewältigung zentraler Herausforderungen in den Bereichen Sicherheit, Datenverwaltung, Ressourcennutzung und Skalierbarkeit können Unternehmen ihre KI-Initiativen beschleunigen und gleichzeitig die Betriebseffizienz, den Datenschutz und die Zusammenarbeit aufrechterhalten.  Der integrierte Ansatz der Lösung beseitigt häufige Engpässe in KI-Entwicklungspipelines und ermöglicht es Datenwissenschaftlern und Ingenieuren, sich auf Innovationen statt auf die Infrastrukturverwaltung zu konzentrieren.



== Wo Sie weitere Informationen finden

Weitere Informationen zu den in diesem Dokument beschriebenen Informationen finden Sie in den folgenden Dokumenten und/oder auf den folgenden Websites:

* https://www.netapp.com/pdf.html?item=/media/125003-nva-1175-design-superpod-a90.pdf["NVA-1175 NVIDIA DGX SuperPOD mit NetApp AFF A90 -Speichersystem-Designhandbuch"]
* https://docs.nvidia.com/dgx-superpod/reference-architecture-scalable-infrastructure-b200/latest/index.html["NVIDIA DGX B200 SuperPOD-Referenzarchitektur"]
* https://docs.nvidia.com/dgx-superpod/reference-architecture/scalable-infrastructure-h200/latest/index.html["+++ NVIDIA DGX H200 SuperPOD-Referenzarchitektur+++"]
* https://docs.nvidia.com/base-command-manager/index.html#product-manuals["+++ NVIDIA BaseCommand Software+++"]
* https://nvdam.widen.net/s/mmvbnpk8qk/networking-ethernet-switches-sn5000-datasheet-us["+++ NVIDIA Spectrum SN5600 Ethernet-Switches+++"]
* https://docs.nvidia.com/dgx-superpod/release-notes/latest/10-24-11.html["+++ NVIDIA DGX SuperPOD Versionshinweise+++"]
* https://docs.netapp.com/us-en/ontap-systems/a70-90/install-overview.html["+++ NetApp AFF A90 Installation+++"]
* https://docs.netapp.com/us-en/netapp-solutions/ai/index.html["+++ Dokumentation zu NetApp KI-Lösungen+++"]
* https://docs.netapp.com/us-en/ontap/index.html["+++ NetApp ONTAP Software+++"]
* https://docs.netapp.com/us-en/ontap-systems/aff-aseries/index.html["+++ NetApp installiert und wartet AFF -Speichersysteme+++"]
* https://docs.netapp.com/us-en/ontap/nfs-rdma/index.html["NFS über RDMA"]
* https://www.netapp.com/media/19761-tr-4063.pdf["+++Was ist pNFS+++"](älteres Dokument mit tollen pNFS-Informationen)




== Anhang A: Stückliste und Rack-Erhöhungen



=== Stückliste

Tabelle 3 zeigt die Teilenummer und Menge der NetApp -Komponenten, die zum Bereitstellen des Speichers für eine, zwei, drei und vier skalierbare Einheiten erforderlich sind.

Tabelle 3) NetApp BOM für 1, 2, 3 und 4 SU.

[cols="20%,32%,12%,12%,12%,12%"]
|===
| Teil # | Artikel | Menge für 1SU | Menge für 2SU | Menge für 3SU | Menge für 4SU 


| AFF-A90A-100-C | AFF A90 Lagersystem | 4 | 8 | 12 | 16 


| X4025A-2-A-C | 2 x 7,6 TB Laufwerkspaket | 48 | 96 | 144 | 192 


| X50131A-C | IO-Modul, 2PT, 100/200/400GbE | 24 | 48 | 96 | 128 


| X50130A-C | IO-Modul, 2PT, 100GbE | 16 | 32 | 48 | 64 


| X-02659-00 | Bausatz, 4 Pfosten, quadratisches oder rundes Loch, 24"-32" Schiene | 4 | 8 | 12 | 16 


| X1558A-R6 | Netzkabel, im Schrank, 48 Zoll, + C13-C14, 10 A/250 V | 20 | 40 | 60 | 80 


| X190200-CS | Cluster-Schalter, N9336C 36Pt PTSX10/25/40/100G | 2 | 4 | 6 | 8 


| X66211A-2 | Kabel, 100GbE, QSFP28-QSFP28, Cu, 2m | 16 | 32 | 48 | 64 


| X66211A-05 | Kabel, 100GbE, QSFP28-QSFP28, Cu, 0,5m | 4 | 8 | 12 | 16 


| X6561-R6 | Kabel, Ethernet, CAT6, RJ45, 5m | 18 | 34 | 50 | 66 
|===
Tabelle 4 zeigt die Teilenummer und Menge der NVIDIA Kabel, die zum Verbinden der AFF A90 Speichersysteme mit den SN5600-Switches in den Hochleistungsspeicher- und In-Band-Netzwerken erforderlich sind.

Tabelle 4) Erforderliche NVIDIA Kabel zum Verbinden von AFF A90 Speichersystemen mit den SN5600-Switches in den Hochleistungsspeicher- und In-Band-Netzwerken.

[cols="20%,32%,12%,12%,12%,12%"]
|===
| Teil # | Artikel | Menge für 1SU | Menge für 2SU | Menge für 3SU | Menge für 4SU 


| MCP7Y40-N003 | DAC 3m 26ga 2x400G bis 4x200G OSFP bis 4xQSFP112 | 12 | 24 | 36 | 48 


| ODER |  |  |  |  |  


| MMS4X00-NS | Doppelport-OSFP 2x400G 2xSR4 Multimode-Transceiver Dual MPO-12/APC | 12 | 24 | 36 | 48 


| MFP7E20-N0XX | Multimode-Faser-Splitter 400G-> 2x200G XX = 03, 05, 07, 10, 15, 20, 30, 40, 50) Meter | 24 | 48 | 96 | 128 


| MMA1Z00-NS400 | Einzelport 400G SR4 Multimode QSFP112Transceiver Einzel MPO-12/APC | 48 | 96 | 144 | 192 
|===


=== Rack-Erhöhungen

Die Abbildungen 4–6 zeigen beispielhafte Rackhöhen für 1–4 SU.

Abbildung 4) Rack-Erhöhungen für 1 SU und 2 SU.

image:ai-superpod-a90-008.png["600.600"]

Abbildung 5) Rack-Erhöhungen für 3 SU.

image:ai-superpod-a90-009.png["600.600"]

Abbildung 6) Rack-Erhöhungen für 4 SU.

image:ai-superpod-a90-010.png["600.600"]
